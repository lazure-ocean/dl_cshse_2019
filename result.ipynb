{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, embedding_matrix):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) \n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(embedding_size, int(hidden_size/2),\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1*2, 1, int(self.hidden_size/2))\n",
    "\n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, embedding_matrix):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MASKED_token = 2\n",
    "MAX_LENGTH = 42\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 1, \"MASKEDTOKEN\": 2}\n",
    "        self.index2word = {0: \"SOSTOKEN\", 1: \"EOSTOKEN\", 2: \"MASKEDTOKEN\"}\n",
    "        self.word2count = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 0, \"MASKEDTOKEN\": 0}\n",
    "        \n",
    "        self.n_words = 3  # Count SOS and EOS and Masked token\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to\n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):  # Lowercase, trim, and remove non-letter characters\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = \" \".join(s.split()[:40])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLang(dataset_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_title: either 'imdb' or 'ptb'\n",
    "    \"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    if dataset_title == 'imdb':\n",
    "        train = imdb_dataset(train=True, directory='../data/')\n",
    "        # Read the dataset and split into lines\n",
    "        lines = [train[ind]['text'].strip() for ind, doc in enumerate(train)]\n",
    "        # Normalize lines\n",
    "        lines = [' '.join([\"SOSTOKEN\", normalizeString(s), \"EOSTOKEN\"]) for s in lines]\n",
    "        lang = Lang(dataset_title)\n",
    "    elif dataset_title == 'ptb':\n",
    "        raise NotImplementedError\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataset_title):\n",
    "    lang, lines = readLang(dataset_title)\n",
    "    print(\"Read %s sentence pairs\" % len(lines))\n",
    "    print(\"Counting words...\")\n",
    "    for l in lines:\n",
    "        lang.addSentence(l)\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    #indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsForTrain(lang, sentence):\n",
    "    # mask = generate_mask(len(sentence))\n",
    "    target_tensor = tensorFromSentence(lang, sentence)\n",
    "    # transformed_sentence = \" \".join(transform_input_with_is_missing_token(sentence.split(), mask))\n",
    "    #input_tensor = tensorFromSentence(lang, transformed_sentence)\n",
    "    return target_tensor # , target_tensor\n",
    "\n",
    "def indexFromTensor(lang, decoder_output):\n",
    "    return decoder_output.max(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 42 # max(map(lambda x: len(x.split()), imdb_lines)) == 2516\n",
    "\n",
    "def train(input_tensor, model, model_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    #c_ = time()\n",
    "    model_hidden = model.initHidden()\n",
    "\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    model_outputs = torch.zeros(max_length, model.input_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        model_output, model_hidden = model(\n",
    "            input_tensor[ei], model_hidden)\n",
    "        #print(model_output, input_tensor.shape, input_tensor[0].shape)\n",
    "        loss += criterion(model_output[0], input_tensor[ei + 1])\n",
    "        model_outputs[ei] = model_output[0]\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    model_optimizer.step()\n",
    "\n",
    "    return loss.item() / input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model, lang, lines, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    #start = time.time()\n",
    "    start = time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    training_sentences = [tensorFromSentence(lang, lines[0]) for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    \n",
    "    for number in range(1, n_iters + 1):\n",
    "        #c_ = time()\n",
    "        input_tensor = training_sentences[number - 1]\n",
    "        #print('Pairs created ...', time() - c_)\n",
    "        #c_ = time()\n",
    "        loss = train(input_tensor, model,\n",
    "                     model_optimizer, criterion)\n",
    "        #print('Loss is done...', time() - c_)\n",
    "        #c_ = time()\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if number % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, number / n_iters),\n",
    "                                         number, number / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if number % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "    return plot_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sequence_length, batch_size=None, is_present=0.7):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    returns: [1, 1, 0, 1, 0, 1]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        mask = np.random.binomial(1, is_present, size=(batch_size, sequence_length))\n",
    "    elif batch_size is None:\n",
    "        mask = np.random.binomial(1, is_present, size=(sequence_length,))\n",
    "    return torch.from_numpy(mask).long().view(len(mask), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:03, 25.6MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "imdb 32736\n",
      "CPU times: user 22.4 s, sys: 3.83 s, total: 26.2 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imdb_lang, imdb_lines = prepareData('imdb')\n",
    "#print(random.choice(imdb_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 1/1 [00:00<00:00, 813.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# mask = generate_mask()\n",
    "# spm_tokenize = tokenize.SentencePieceTokenizer(args.spm_prefix)\n",
    "\n",
    "# # Compute Batch Size\n",
    "# max_tokens_per_device = 48000\n",
    "# # max_tokens_per_device = 1000\n",
    "# n_devices = torch.cuda.device_count()\n",
    "# max_tokens = max_tokens_per_device * n_devices\n",
    "# truncate_length = 20\n",
    "# batch_size = int(max_tokens/truncate_length)\n",
    "\n",
    "# checkpoint_path = \"/home/ipyaternev/dl_cshse_2019/\"\n",
    "# saver = Saver(checkpoint_path)\n",
    "\n",
    "# train_path = os.path.join(args.path, 'train')\n",
    "# dev_path = os.path.join(args.path, 'test')\n",
    "\n",
    "# train_dataset = \n",
    "\n",
    "# # Constructed vocabulary from train\n",
    "# vocab = train_dataset.vocab\n",
    "# Task = namedtuple('Task', 'source_dictionary target_dictionary')\n",
    "# task = Task(source_dictionary=vocab, \n",
    "#         target_dictionary=vocab)\n",
    "\n",
    "# trainer = MGANTrainer(args, task, saver, visdom, vocab)\n",
    "# def loader(dataset):\n",
    "#     _loader = DataLoader(dataset, batch_size=batch_size, \n",
    "#             collate_fn=TensorIMDbDataset.collate, \n",
    "#             shuffle=True, num_workers=8)\n",
    "#     return _loader\n",
    "\n",
    "# #trainer.validate_dataset(loader(train_dataset))\n",
    "\n",
    "# dev_dataset = TensorIMDbDataset(\n",
    "#         dev_path, spm_tokenize, \n",
    "#         rmask, truncate_length,\n",
    "#         vocab \n",
    "# )\n",
    "\n",
    "# Datasets = namedtuple('Dataset', 'train dev')\n",
    "# datasets = Datasets(\n",
    "#         train=train_dataset,\n",
    "#         dev=dev_dataset\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(args.max_epochs), total=args.max_epochs, desc='epoch'):\n",
    "#     train_loader = loader(datasets.train)\n",
    "#     pbar = tqdm(train_loader, desc='training', leave=True)\n",
    "#     for i, samples in enumerate(pbar):\n",
    "#         trainer.run(epoch, samples)\n",
    "#         if i % args.validate_every == 0:\n",
    "#             validation_samples = 1000\n",
    "#             val_idxs = random.sample(range(len(datasets.dev)), validation_samples)\n",
    "#             subset = torch.utils.data.Subset(datasets.dev, val_idxs)\n",
    "#             trainer.validate_dataset(loader(subset))\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), total=max_epochs, desc='epoch'):\n",
    "    \n",
    "    #model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    training_sentences = [tensorFromSentence(imdb_lang, imdb_lines[0]) for i in range(10)]\n",
    "    masks = [generate_mask(el.size()[0]) for el in training_sentences]\n",
    "    masked_sentences = [s * t for (s, t) in zip(training_sentences, masks)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#    criterion = critic()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  4,  5,  6,  7,  8,  9, 10, 11,  5, 12, 13, 14, 15, 16,  3, 17,\n",
       "        18, 19, 20, 21, 22, 23,  3, 24, 25, 26, 27, 28, 29, 30, 10,  5, 31,  3,\n",
       "        32, 33, 34,  3, 35,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[0].view(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 6],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [ 0],\n",
       "        [24],\n",
       "        [ 0],\n",
       "        [26],\n",
       "        [27],\n",
       "        [28],\n",
       "        [29],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 5],\n",
       "        [31],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [33],\n",
       "        [34],\n",
       "        [ 3],\n",
       "        [35],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [ 5],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [ 3],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [ 3],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [10],\n",
       "         [ 5],\n",
       "         [31],\n",
       "         [ 3],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [ 3],\n",
       "         [35],\n",
       "         [ 1]])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(hparams, inputs, targets, present, is_training, is_validating, reuse=None):\n",
    "    '''\n",
    "    Use seq2seq_vd from tf repo\n",
    "    ''' \n",
    "    sequence, logits, log_probs, initial_state, final_state = seq2seq.generator(hparams, \n",
    "                                                                                  inputs, \n",
    "                                                                                  targets, \n",
    "                                                                                  present, \n",
    "                                                                                  is_training=is_training, \n",
    "                                                                                  is_validating=is_validating, \n",
    "                                                                                  reuse=reuse)\n",
    "    return (sequence, logits, log_probs, initial_state, final_state, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(hparams,\n",
    "                         sequence,\n",
    "                         is_training,\n",
    "                         reuse=None,\n",
    "                         initial_state=None,\n",
    "                         inputs=None,\n",
    "                         present=None):\n",
    "    \n",
    "    '''Use seq2seq_vd (maybe birectional) from tf repo'''\n",
    "    \n",
    "    if FLAGS.discriminator_model == 'seq2seq_vd':\n",
    "        predictions = seq2seq_vd.discriminator(\n",
    "                                                hparams,\n",
    "                                                inputs,\n",
    "                                                present,\n",
    "                                                sequence,\n",
    "                                                is_training=is_training,\n",
    "                                                reuse=reuse)\n",
    "    elif FLAGS.discriminator_model == 'bidirectional_vd':\n",
    "        predictions = bidirectional_vd.discriminator(\n",
    "                                                hparams,\n",
    "                                                sequence,\n",
    "                                                is_training=is_training,\n",
    "                                                reuse=reuse,\n",
    "                                                initial_state=initial_state)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(hparams, sequence, is_training, reuse=None):\n",
    "    \"\"\"Create the Critic model specified by the FLAGS and hparams.\n",
    "    Args:\n",
    "    hparams:  Hyperparameters for the MaskGAN.\n",
    "    sequence:  tf.int32 Tensor sequence of shape [batch_size, sequence_length]\n",
    "    is_training:  Whether the model is training.\n",
    "    reuse (Optional):  Whether to reuse the model.\n",
    "    Returns:\n",
    "    values:  tf.float32 Tensor of predictions of shape [batch_size,\n",
    "      sequence_length]\n",
    "    \"\"\"\n",
    "    if FLAGS.baseline_method == 'critic':\n",
    "        if FLAGS.discriminator_model == 'seq2seq_vd':\n",
    "            values = critic_vd.critic_seq2seq_vd_derivative(\n",
    "                  hparams, sequence, is_training, reuse=reuse)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, cell, batch_size, input_size, recurrent_keep_prob, input_keep_prob):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "        \n",
    "        self._cell = cell\n",
    "        self._recurrent_keep_prob = recurrent_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "        \n",
    "        def make_mask(keep_prob, units):\n",
    "            random_tensor = keep_prob\n",
    "            random_tensor += torch.random(torch.)\n",
    "        \n",
    "    def kl(self):\n",
    "        c1 = 1.16145124\n",
    "        c2 = -1.50204118\n",
    "        c3 = 0.58629921\n",
    "        \n",
    "        alpha = self.log_alpha.exp()\n",
    "        \n",
    "        negative_kl = 0.5 * self.log_alpha + c1 * alpha + c2 * alpha**2 + c3 * alpha**3\n",
    "        \n",
    "        kl = -negative_kl\n",
    "        \n",
    "        return kl.mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise   e ~ N(1, alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            # N(0,1)\n",
    "            epsilon = torch.randn(x.size())\n",
    "            if x.is_cuda:\n",
    "                epsilon = epsilon.cuda()\n",
    "\n",
    "            # Clip alpha\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_alpha)\n",
    "            alpha = self.log_alpha.exp()\n",
    "\n",
    "            # N(1, alpha)\n",
    "            epsilon = epsilon * alpha\n",
    "\n",
    "            return x * epsilon\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(logger, vocab, masked, unmasked, generated, truncate=None):\n",
    "    sequence_generator = SequenceGenerator(vocab)\n",
    "    masked = sequence_generator(masked)\n",
    "    unmasked = sequence_generator(unmasked)\n",
    "    generated = sequence_generator(generated)\n",
    "    lines = []\n",
    "    truncate = truncate if truncate is not None else len(masked)\n",
    "    for _masked, _unmasked, _generated in zip(masked, unmasked, generated):\n",
    "        lines.append('> {}'.format(_masked))\n",
    "        lines.append('< {}'.format(_generated))\n",
    "        lines.append('= {}'.format(_unmasked))\n",
    "        lines.append(\"\")\n",
    "        truncate -= 1\n",
    "        if truncate <= 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = createMaskGAN(parameters, is_training)\n",
    "\n",
    "# Run the requested number of evaluation steps\n",
    "avg_epoch_gen_loss, avg_epoch_dis_loss = [], []\n",
    "cumulative_costs = 0.\n",
    "\n",
    "# Average percent captured for each of the n-grams.\n",
    "avg_percent_captured = {'2': 0., '3': 0., '4': 0.}\n",
    "\n",
    "# Set a random seed to keep fixed mask.\n",
    "np.random.seed(0)\n",
    "gen_iters = 0\n",
    "iterator = get_iterator(data)\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model = rollout.create_rollout_MaskGAN(hparams, is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple, defaultdict\n",
    "from torch.nn.parallel import DataParallel\n",
    "# ЗАменить на функцию создания модели\n",
    "#from .distributed_model import MGANModel\n",
    "# Добавил выше\n",
    "#from mgan.utils.sequence_recovery import pretty_print\n",
    "# Добавил выше\n",
    "#from mgan.optim import ClippedAdam\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# Заменить самописным или спереть из fairseq?\n",
    "#from fairseq.meters import AverageMeter\n",
    "\n",
    "def _tqdm(length, desc):\n",
    "    pbar = tqdm(\n",
    "        range(length), total=length,\n",
    "        leave=True, desc=desc\n",
    "    )\n",
    "    return pbar\n",
    "\n",
    "class MGANTrainer:\n",
    "    def __init__(self, args, task, saver, logger, vocab):\n",
    "        device = torch.device(\"cuda\")\n",
    "        self.pretrain = False\n",
    "        self.saver = saver\n",
    "        self.logger = logger\n",
    "        self._model = MGANModel.build_model(args, task, pretrain=self.pretrain)\n",
    "        #self.model = DataParallel(self._model)\n",
    "        self.model = self.model.to(device)\n",
    "        self.opt = ClippedAdam(self.model.parameters(), lr=1e-3)\n",
    "        self.opt.set_clip(clip_value=5.0)\n",
    "        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(self.opt, gamma=0.5)\n",
    "        self.saver.load(\"mgan\", self.model.module)\n",
    "        self.step = 0\n",
    "        self.vocab = vocab\n",
    "        self.critic_lag_max = 50\n",
    "        self.critic_lag = self.critic_lag_max \n",
    "\n",
    "        self.args = args\n",
    "        self.task = task\n",
    "\n",
    "\n",
    "    def forward(self, epoch, samples):\n",
    "        self.model.train()\n",
    "        num_rollouts = 1 if self.pretrain else self.args.num_rollouts\n",
    "        self.lr_scheduler.step(epoch)\n",
    "        self.rollout_discriminator(num_rollouts, samples)\n",
    "        self.rollout_generator(num_rollouts, samples)\n",
    "        self.rollout_critic(num_rollouts, samples)\n",
    "        self.saver.checkpoint(\"mgan\", self.model.module)\n",
    "        self.step += 1\n",
    "\n",
    "    def rollout_discriminator(self, num_rollouts, samples):\n",
    "        masked, unmasked, lengths, mask = samples\n",
    "        real, fake = torch.zeros(unm)\n",
    "        batch_size, seq_len = samples[0].size()\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        pbar = _tqdm(num_rollouts, 'discriminator-rollout')\n",
    "\n",
    "        for rollout in pbar:\n",
    "            real_loss = self.model(\n",
    "                    masked, lengths, mask, unmasked, \n",
    "                    tag=\"d-step\", real=True\n",
    "            )\n",
    "\n",
    "            real_loss = real_loss.sum()/batch_size\n",
    "\n",
    "            with torch.no_grad():\n",
    "                net_output = self.model(\n",
    "                        masked, lengths, mask, \n",
    "                        unmasked, tag=\"g-step\"\n",
    "                )\n",
    "                generated = net_output[1]\n",
    "\n",
    "            fake_loss = self.model(\n",
    "                    masked, lengths, mask, generated, \n",
    "                    tag=\"d-step\", real=False\n",
    "            )\n",
    "\n",
    "            fake_loss = fake_loss.sum()/batch_size\n",
    "\n",
    "            loss = (real_loss + fake_loss)/2\n",
    "            loss.backward()\n",
    "\n",
    "            real.update(real_loss.item())\n",
    "            fake.update(fake_loss.item())\n",
    "\n",
    "        self.opt.step()\n",
    "        self.logger.log(\"discriminator/real\", self.step, real.avg)\n",
    "        self.logger.log(\"discriminator/fake\", self.step, fake.avg)\n",
    "        self.logger.log(\"discriminator\",      self.step, real.avg + fake.avg)\n",
    "\n",
    "    def rollout_critic(self, num_rollouts, samples):\n",
    "        masked, unmasked, lengths, mask = samples\n",
    "        batch_size, seq_len = samples[0].size()\n",
    "        meter = AverageMeter()\n",
    "        self.opt.zero_grad()\n",
    "        pbar = _tqdm(num_rollouts, 'critic-rollout')\n",
    "        for rollout in pbar:\n",
    "            loss = self.model(masked, lengths, mask, unmasked, tag=\"c-step\")\n",
    "            loss = loss.sum() / batch_size\n",
    "            loss.backward()\n",
    "            meter.update(loss.item())\n",
    "\n",
    "        self.opt.step()\n",
    "        self.logger.log(\"critic/loss\", self.step, meter.avg)\n",
    "\n",
    "    \n",
    "    def rollout_generator(self, num_rollouts, samples):\n",
    "        masked, unmasked, lengths, mask = samples\n",
    "        batch_size, seq_len = samples[0].size()\n",
    "        meter = AverageMeter()\n",
    "        ppl_meter = defaultdict(lambda: AverageMeter())\n",
    "        self.opt.zero_grad()\n",
    "        pbar = _tqdm(num_rollouts, 'generator-rollout')\n",
    "\n",
    "        for rollout in pbar:\n",
    "            loss, generated, ppl = self.model(masked, lengths, mask, unmasked, tag=\"g-step\")\n",
    "            loss = loss.sum() / batch_size\n",
    "            loss.backward()\n",
    "            meter.update(-1*loss.item())\n",
    "            # for key in ppl:\n",
    "            #     ppl[key] = ppl[key].sum() / batch_size\n",
    "            #     ppl_meter[key].update(ppl[key].item())\n",
    "        self.opt.step()\n",
    "        self.logger.log(\"generator/advantage\", self.step, meter.avg)\n",
    "        # for key in ppl_meter:\n",
    "        #     self.logger.log(\"ppl/{}\".format(key), ppl_meter[key].avg)\n",
    "\n",
    "        self.debug('train', samples, generated)\n",
    "\n",
    "    def debug(self, key, samples, generated):\n",
    "        masked, unmasked, lengths, mask = samples\n",
    "        tag = 'generated/{}'.format(key)\n",
    "        logger = lambda s: self.logger.log(tag, s)\n",
    "        pretty_print(logger, self.vocab, masked, unmasked, generated, truncate=10)\n",
    "\n",
    "    def validate_dataset(self, loader):\n",
    "        self.model.eval()\n",
    "        _meters = 'generator dfake dreal critic ppl_sampled ppl_truths'\n",
    "        _n_meters = len(_meters.split())\n",
    "        Meters = namedtuple('Meters', _meters)\n",
    "        meters_list =  [AverageMeter() for i in range(_n_meters)]\n",
    "        meters = Meters(*meters_list)\n",
    "        for sample_batch in loader:\n",
    "            self._validate(meters, sample_batch)\n",
    "            for key, value in meters._asdict().items():\n",
    "                pass\n",
    "                # print(key, value.avg)\n",
    "\n",
    "    @property\n",
    "    def umodel(self):\n",
    "        if isinstance(self.model, DataParallel):\n",
    "            return self.model.module\n",
    "        return self.model\n",
    "\n",
    "    def aggregate(self, batch_size):\n",
    "        return lambda tensor: tensor.sum() / batch_size\n",
    "\n",
    "    def _validate(self, meters, samples):\n",
    "        with torch.no_grad():\n",
    "            masked, unmasked, lengths, mask = samples\n",
    "            batch_size, seq_len = samples[0].size()\n",
    "\n",
    "            agg = self.aggregate(batch_size)\n",
    "\n",
    "            real_loss = self.model(\n",
    "                    masked, lengths, mask, unmasked, \n",
    "                    tag=\"d-step\", real=True\n",
    "            )\n",
    "\n",
    "            real_loss = agg(real_loss)\n",
    "\n",
    "            generator_loss, generated, ppl = self.model(\n",
    "                    masked, lengths, mask, \n",
    "                    unmasked, tag=\"g-step\",\n",
    "                    ppl=True\n",
    "            )\n",
    "\n",
    "            generator_loss = agg(generator_loss)\n",
    "\n",
    "            fake_loss = self.model(\n",
    "                    masked, lengths, mask, generated, \n",
    "                    tag=\"d-step\", real=False\n",
    "            )\n",
    "\n",
    "            fake_loss = agg(fake_loss)\n",
    "\n",
    "            loss = (real_loss + fake_loss)/2\n",
    "\n",
    "            critic_loss = self.model(masked, lengths, mask, unmasked, tag=\"c-step\")\n",
    "            critic_loss = agg(fake_loss)\n",
    "\n",
    "            meters.dreal.update(real_loss.item())\n",
    "            meters.dfake.update(fake_loss.item())\n",
    "            meters.generator.update(generator_loss.item())\n",
    "            meters.critic.update(critic_loss.item())\n",
    "\n",
    "            self.debug('dev', samples, generated)\n",
    "\n",
    "            for key in ppl:\n",
    "                ppl[key] = agg(ppl[key])\n",
    "\n",
    "            meters.ppl_sampled.update(ppl['sampled'].item())\n",
    "            meters.ppl_truths.update(ppl['ground-truth'].item())\n",
    "            self.debug('dev', samples, generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader(dataset, batch_size=100):\n",
    "        _loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                collate_fn=TensorIMDbDataset.collate, \n",
    "                shuffle=True, num_workers=-1)\n",
    "        return _loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "or epoch in tqdm(range(args.max_epochs), total=args.max_epochs, desc='epoch'):\n",
    "        \n",
    "        pbar = tqdm(train_dataset, desc='training', leave=True)\n",
    "        for i, samples in enumerate(pbar):\n",
    "            trainer.run(epoch, samples)\n",
    "            if i % args.validate_every == 0:\n",
    "                validation_samples = 1000\n",
    "                val_idxs = random.sample(range(len(datasets.dev)), validation_samples)\n",
    "                subset = torch.utils.data.Subset(datasets.dev, val_idxs)\n",
    "                trainer.validate_dataset(loader(subset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
