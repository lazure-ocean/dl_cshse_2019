{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, embedding_matrix):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) \n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(embedding_size, int(hidden_size/2),\n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1,1,-1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1*2, 1, int(self.hidden_size/2))\n",
    "\n",
    "class BiLSTMDecoder(nn.Module):\n",
    "    def __init__(self, output_size, embedding_size, hidden_size, embedding_matrix):\n",
    "        super(BiLSTMDecoder, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # layers\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1,1,-1)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MASKED_token = 2\n",
    "MAX_LENGTH = 42\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 1, \"MASKEDTOKEN\": 2}\n",
    "        self.index2word = {0: \"SOSTOKEN\", 1: \"EOSTOKEN\", 2: \"MASKEDTOKEN\"}\n",
    "        self.word2count = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 0, \"MASKEDTOKEN\": 0}\n",
    "        \n",
    "        self.n_words = 3  # Count SOS and EOS and Masked token\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to\n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):  # Lowercase, trim, and remove non-letter characters\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = \" \".join(s.split()[:40])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLang(dataset_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_title: either 'imdb' or 'ptb'\n",
    "    \"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    if dataset_title == 'imdb':\n",
    "        train = imdb_dataset(train=True, directory='../data/')\n",
    "        # Read the dataset and split into lines\n",
    "        lines = [train[ind]['text'].strip() for ind, doc in enumerate(train)]\n",
    "        # Normalize lines\n",
    "        lines = [' '.join([\"SOSTOKEN\", normalizeString(s), \"EOSTOKEN\"]) for s in lines]\n",
    "        lang = Lang(dataset_title)\n",
    "    elif dataset_title == 'ptb':\n",
    "        raise NotImplementedError\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataset_title):\n",
    "    lang, lines = readLang(dataset_title)\n",
    "    print(\"Read %s sentence pairs\" % len(lines))\n",
    "    print(\"Counting words...\")\n",
    "    for l in lines:\n",
    "        lang.addSentence(l)\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    #indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsForTrain(lang, sentence):\n",
    "    # mask = generate_mask(len(sentence))\n",
    "    target_tensor = tensorFromSentence(lang, sentence)\n",
    "    # transformed_sentence = \" \".join(transform_input_with_is_missing_token(sentence.split(), mask))\n",
    "    #input_tensor = tensorFromSentence(lang, transformed_sentence)\n",
    "    return target_tensor # , target_tensor\n",
    "\n",
    "def indexFromTensor(lang, decoder_output):\n",
    "    return decoder_output.max(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 42 # max(map(lambda x: len(x.split()), imdb_lines)) == 2516\n",
    "\n",
    "def train(input_tensor, model, model_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    #c_ = time()\n",
    "    model_hidden = model.initHidden()\n",
    "\n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    model_outputs = torch.zeros(max_length, model.input_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length - 1):\n",
    "        model_output, model_hidden = model(\n",
    "            input_tensor[ei], model_hidden)\n",
    "        #print(model_output, input_tensor.shape, input_tensor[0].shape)\n",
    "        loss += criterion(model_output[0], input_tensor[ei + 1])\n",
    "        model_outputs[ei] = model_output[0]\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    model_optimizer.step()\n",
    "\n",
    "    return loss.item() / input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sequence_length, batch_size=None, is_present=0.7):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    returns: [1, 1, 0, 1, 0, 1]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        mask = np.random.binomial(1, is_present, size=(batch_size, sequence_length))\n",
    "    elif batch_size is None:\n",
    "        mask = np.random.binomial(1, is_present, size=(sequence_length,))\n",
    "    return torch.from_numpy(mask).long().view(len(mask), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 0.00B [00:00, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:03, 25.6MB/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "imdb 32736\n",
      "CPU times: user 22.4 s, sys: 3.83 s, total: 26.2 s\n",
      "Wall time: 29.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "imdb_lang, imdb_lines = prepareData('imdb')\n",
    "#print(random.choice(imdb_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 1/1 [00:00<00:00, 813.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# mask = generate_mask()\n",
    "# spm_tokenize = tokenize.SentencePieceTokenizer(args.spm_prefix)\n",
    "\n",
    "# # Compute Batch Size\n",
    "# max_tokens_per_device = 48000\n",
    "# # max_tokens_per_device = 1000\n",
    "# n_devices = torch.cuda.device_count()\n",
    "# max_tokens = max_tokens_per_device * n_devices\n",
    "# truncate_length = 20\n",
    "# batch_size = int(max_tokens/truncate_length)\n",
    "\n",
    "# checkpoint_path = \"/home/ipyaternev/dl_cshse_2019/\"\n",
    "# saver = Saver(checkpoint_path)\n",
    "\n",
    "# train_path = os.path.join(args.path, 'train')\n",
    "# dev_path = os.path.join(args.path, 'test')\n",
    "\n",
    "# train_dataset = \n",
    "\n",
    "# # Constructed vocabulary from train\n",
    "# vocab = train_dataset.vocab\n",
    "# Task = namedtuple('Task', 'source_dictionary target_dictionary')\n",
    "# task = Task(source_dictionary=vocab, \n",
    "#         target_dictionary=vocab)\n",
    "\n",
    "# trainer = MGANTrainer(args, task, saver, visdom, vocab)\n",
    "# def loader(dataset):\n",
    "#     _loader = DataLoader(dataset, batch_size=batch_size, \n",
    "#             collate_fn=TensorIMDbDataset.collate, \n",
    "#             shuffle=True, num_workers=8)\n",
    "#     return _loader\n",
    "\n",
    "# #trainer.validate_dataset(loader(train_dataset))\n",
    "\n",
    "# dev_dataset = TensorIMDbDataset(\n",
    "#         dev_path, spm_tokenize, \n",
    "#         rmask, truncate_length,\n",
    "#         vocab \n",
    "# )\n",
    "\n",
    "# Datasets = namedtuple('Dataset', 'train dev')\n",
    "# datasets = Datasets(\n",
    "#         train=train_dataset,\n",
    "#         dev=dev_dataset\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in tqdm(range(args.max_epochs), total=args.max_epochs, desc='epoch'):\n",
    "#     train_loader = loader(datasets.train)\n",
    "#     pbar = tqdm(train_loader, desc='training', leave=True)\n",
    "#     for i, samples in enumerate(pbar):\n",
    "#         trainer.run(epoch, samples)\n",
    "#         if i % args.validate_every == 0:\n",
    "#             validation_samples = 1000\n",
    "#             val_idxs = random.sample(range(len(datasets.dev)), validation_samples)\n",
    "#             subset = torch.utils.data.Subset(datasets.dev, val_idxs)\n",
    "#             trainer.validate_dataset(loader(subset))\n",
    "\n",
    "max_epochs = 1\n",
    "\n",
    "for epoch in tqdm(range(max_epochs), total=max_epochs, desc='epoch'):\n",
    "    \n",
    "    #model_optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    training_sentences = [tensorFromSentence(imdb_lang, imdb_lines[0]) for i in range(10)]\n",
    "    masks = [generate_mask(el.size()[0]) for el in training_sentences]\n",
    "    masked_sentences = [s * t for (s, t) in zip(training_sentences, masks)]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#    criterion = critic()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  3,  4,  5,  6,  7,  8,  9, 10, 11,  5, 12, 13, 14, 15, 16,  3, 17,\n",
       "        18, 19, 20, 21, 22, 23,  3, 24, 25, 26, 27, 28, 29, 30, 10,  5, 31,  3,\n",
       "        32, 33, 34,  3, 35,  1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[0].view(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0],\n",
       "        [ 3],\n",
       "        [ 4],\n",
       "        [ 0],\n",
       "        [ 6],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 9],\n",
       "        [10],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [12],\n",
       "        [13],\n",
       "        [14],\n",
       "        [15],\n",
       "        [16],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [19],\n",
       "        [20],\n",
       "        [21],\n",
       "        [22],\n",
       "        [23],\n",
       "        [ 0],\n",
       "        [24],\n",
       "        [ 0],\n",
       "        [26],\n",
       "        [27],\n",
       "        [28],\n",
       "        [29],\n",
       "        [ 0],\n",
       "        [10],\n",
       "        [ 5],\n",
       "        [31],\n",
       "        [ 3],\n",
       "        [ 0],\n",
       "        [33],\n",
       "        [34],\n",
       "        [ 3],\n",
       "        [35],\n",
       "        [ 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[3].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7],\n",
       "         [ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11],\n",
       "         [ 5],\n",
       "         [12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15],\n",
       "         [16],\n",
       "         [ 3],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19],\n",
       "         [20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23],\n",
       "         [ 3],\n",
       "         [24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27],\n",
       "         [28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [10],\n",
       "         [ 5],\n",
       "         [31],\n",
       "         [ 3],\n",
       "         [32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [ 3],\n",
       "         [35],\n",
       "         [ 1]])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator(hparams, inputs, targets, present, is_training, is_validating, reuse=None):\n",
    "    '''\n",
    "    Use seq2seq_vd from tf repo\n",
    "    ''' \n",
    "    sequence, logits, log_probs, initial_state, final_state = seq2seq.generator(hparams, \n",
    "                                                                                  inputs, \n",
    "                                                                                  targets, \n",
    "                                                                                  present, \n",
    "                                                                                  is_training=is_training, \n",
    "                                                                                  is_validating=is_validating, \n",
    "                                                                                  reuse=reuse)\n",
    "    return (sequence, logits, log_probs, initial_state, final_state, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator(hparams,\n",
    "                         sequence,\n",
    "                         is_training,\n",
    "                         reuse=None,\n",
    "                         initial_state=None,\n",
    "                         inputs=None,\n",
    "                         present=None):\n",
    "    \n",
    "    '''Use seq2seq_vd (maybe birectional) from tf repo'''\n",
    "    \n",
    "    if FLAGS.discriminator_model == 'seq2seq_vd':\n",
    "        predictions = seq2seq_vd.discriminator(\n",
    "                                                hparams,\n",
    "                                                inputs,\n",
    "                                                present,\n",
    "                                                sequence,\n",
    "                                                is_training=is_training,\n",
    "                                                reuse=reuse)\n",
    "    elif FLAGS.discriminator_model == 'bidirectional_vd':\n",
    "        predictions = bidirectional_vd.discriminator(\n",
    "                                                hparams,\n",
    "                                                sequence,\n",
    "                                                is_training=is_training,\n",
    "                                                reuse=reuse,\n",
    "                                                initial_state=initial_state)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_critic(hparams, sequence, is_training, reuse=None):\n",
    "    \"\"\"Create the Critic model specified by the FLAGS and hparams.\n",
    "    Args:\n",
    "    hparams:  Hyperparameters for the MaskGAN.\n",
    "    sequence:  tf.int32 Tensor sequence of shape [batch_size, sequence_length]\n",
    "    is_training:  Whether the model is training.\n",
    "    reuse (Optional):  Whether to reuse the model.\n",
    "    Returns:\n",
    "    values:  tf.float32 Tensor of predictions of shape [batch_size,\n",
    "      sequence_length]\n",
    "    \"\"\"\n",
    "    if FLAGS.baseline_method == 'critic':\n",
    "        if FLAGS.discriminator_model == 'seq2seq_vd':\n",
    "            values = critic_vd.critic_seq2seq_vd_derivative(\n",
    "                  hparams, sequence, is_training, reuse=reuse)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropoutWrapper(tf.contrib.rnn.RNNCell):\n",
    "  \"\"\"Add variational dropout to a RNN cell.\"\"\"\n",
    "\n",
    "      def __init__(self, cell, batch_size, input_size, recurrent_keep_prob,\n",
    "                   input_keep_prob):\n",
    "        self._cell = cell\n",
    "        self._recurrent_keep_prob = recurrent_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "\n",
    "        def make_mask(keep_prob, units):\n",
    "          random_tensor = keep_prob\n",
    "          # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\n",
    "          random_tensor += tf.random_uniform(tf.stack([batch_size, units]))\n",
    "          return tf.floor(random_tensor) / keep_prob\n",
    "\n",
    "        self._recurrent_mask = make_mask(recurrent_keep_prob,\n",
    "                                         self._cell.state_size[0])\n",
    "        self._input_mask = self._recurrent_mask\n",
    "\n",
    "      @property\n",
    "      def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "      @property\n",
    "      def output_size(self):\n",
    "        return self._cell.output_size\n",
    "\n",
    "      def __call__(self, inputs, state, scope=None):\n",
    "        dropped_inputs = inputs * self._input_mask\n",
    "        dropped_state = (state[0], state[1] * self._recurrent_mask)\n",
    "        new_h, new_state = self._cell(dropped_inputs, dropped_state, scope)\n",
    "        return new_h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalDropout(nn.Module):\n",
    "    def __init__(self, cell, batch_size, input_size, recurrent_keep_prob, input_keep_prob):\n",
    "        super(VariationalDropout, self).__init__()\n",
    "        \n",
    "        self._cell = cell\n",
    "        self._recurrent_keep_prob = recurrent_keep_prob\n",
    "        self._input_keep_prob = input_keep_prob\n",
    "        \n",
    "        def make_mask(keep_prob, units):\n",
    "            random_tensor = keep_prob\n",
    "            random_tensor += torch.random(torch.)\n",
    "        \n",
    "    def kl(self):\n",
    "        c1 = 1.16145124\n",
    "        c2 = -1.50204118\n",
    "        c3 = 0.58629921\n",
    "        \n",
    "        alpha = self.log_alpha.exp()\n",
    "        \n",
    "        negative_kl = 0.5 * self.log_alpha + c1 * alpha + c2 * alpha**2 + c3 * alpha**3\n",
    "        \n",
    "        kl = -negative_kl\n",
    "        \n",
    "        return kl.mean()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Sample noise   e ~ N(1, alpha)\n",
    "        Multiply noise h = h_ * e\n",
    "        \"\"\"\n",
    "        if self.train():\n",
    "            # N(0,1)\n",
    "            epsilon = torch.randn(x.size())\n",
    "            if x.is_cuda:\n",
    "                epsilon = epsilon.cuda()\n",
    "\n",
    "            # Clip alpha\n",
    "            self.log_alpha.data = torch.clamp(self.log_alpha.data, max=self.max_alpha)\n",
    "            alpha = self.log_alpha.exp()\n",
    "\n",
    "            # N(1, alpha)\n",
    "            epsilon = epsilon * alpha\n",
    "\n",
    "            return x * epsilon\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
