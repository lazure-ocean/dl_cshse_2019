{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import io\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class IMDB(Dataset):\n",
    "\n",
    "    urls = ['http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz']\n",
    "    name = 'imdb'\n",
    "    dirname = 'aclImdb'\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.text)\n",
    "\n",
    "    def __init__(self, path, text_field, label_field, **kwargs):\n",
    "        \"\"\"Create an IMDB dataset instance given a path and fields.\n",
    "        Arguments:\n",
    "            path: Path to the dataset's highest level directory\n",
    "            text_field: The field that will be used for text data.\n",
    "            label_field: The field that will be used for label data.\n",
    "            Remaining keyword arguments: Passed to the constructor of\n",
    "                data.Dataset.\n",
    "        \"\"\"\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "\n",
    "        for label in ['pos', 'neg']:\n",
    "            for fname in glob.iglob(os.path.join(path, label, '*.txt')):\n",
    "                with io.open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "                    text = f.readline()\n",
    "                examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super(IMDB, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, label_field, root='.data',\n",
    "               train='train', test='test', **kwargs):\n",
    "        \"\"\"Create dataset objects for splits of the IMDB dataset.\n",
    "        Arguments:\n",
    "            text_field: The field that will be used for the sentence.\n",
    "            label_field: The field that will be used for label data.\n",
    "            root: Root dataset storage directory. Default is '.data'.\n",
    "            train: The directory that contains the training examples\n",
    "            test: The directory that contains the test examples\n",
    "            Remaining keyword arguments: Passed to the splits method of\n",
    "                Dataset.\n",
    "        \"\"\"\n",
    "        return super(IMDB, cls).splits(\n",
    "            root=root, text_field=text_field, label_field=label_field,\n",
    "            train=train, validation=None, test=test, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def iters(cls, batch_size=32, device=0, root='.data', vectors=None, **kwargs):\n",
    "        \"\"\"Create iterator objects for splits of the IMDB dataset.\n",
    "        Arguments:\n",
    "            batch_size: Batch_size\n",
    "            device: Device to create batches on. Use - 1 for CPU and None for\n",
    "                the currently active GPU device.\n",
    "            root: The root directory that contains the imdb dataset subdirectory\n",
    "            vectors: one of the available pretrained vectors or a list with each\n",
    "                element one of the available pretrained vectors (see Vocab.load_vectors)\n",
    "            Remaining keyword arguments: Passed to the splits method.\n",
    "        \"\"\"\n",
    "        TEXT = data.Field()\n",
    "        LABEL = data.Field(sequential=False)\n",
    "\n",
    "        train, test = cls.splits(TEXT, LABEL, root=root, **kwargs)\n",
    "\n",
    "        TEXT.build_vocab(train, vectors=vectors)\n",
    "        LABEL.build_vocab(train)\n",
    "\n",
    "        return data.BucketIterator.splits(\n",
    "            (train, test), batch_size=batch_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'text_field' and 'label_field'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a5307b7b9e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIMDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/aclImdb/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'text_field' and 'label_field'"
     ]
    }
   ],
   "source": [
    "dataset = IMDB('data/aclImdb/', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
