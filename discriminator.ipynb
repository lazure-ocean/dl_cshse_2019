{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorEncoder(EncoderRNN):\n",
    "    pass\n",
    "\n",
    "class DiscriminatorDecoder(DecoderRNN):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        out_embed_dim =  self.hidden_size\n",
    "        self.fc_out = nn.Linear(out_embed_dim, 1)\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out_dict):\n",
    "        x, attn_scores = super().forward(prev_output_tokens, encoder_out_dict)\n",
    "        return x, attn_scores\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super.__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n",
    "        encoder_out = self.encoder(src_tokens, src_lengths)\n",
    "        decoder_out = self.decoder(prev_output_tokens, encoder_out)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base import Model, Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorEncoder(Encoder):\n",
    "    pass\n",
    "\n",
    "class DiscriminatorDecoder(Decoder):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        out_embed_dim =  self.hidden_size\n",
    "        self.fc_out = nn.Linear(out_embed_dim, 1)\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out_dict):\n",
    "        x, attn_scores = super().forward(prev_output_tokens, encoder_out_dict)\n",
    "        return x, attn_scores\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super.__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src_tokens, src_lengths, prev_output_tokens):\n",
    "        encoder_out = self.encoder(src_tokens, src_lengths)\n",
    "        decoder_out = self.decoder(prev_output_tokens, encoder_out)\n",
    "        return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def discriminator_loss(predictions, labels, missing_tokens):\n",
    "  \"\"\"Discriminator loss based on predictions and labels.\n",
    "  Args:\n",
    "    predictions:  Discriminator linear predictions Tensor of shape [batch_size,\n",
    "      sequence_length]\n",
    "    labels: Labels for predictions, Tensor of shape [batch_size,\n",
    "      sequence_length]\n",
    "    missing_tokens:  Indicator for the missing tokens.  Evaluate the loss only\n",
    "      on the tokens that were missing.\n",
    "  Returns:\n",
    "    loss:  Scalar tf.float32 loss.\n",
    "  \"\"\"\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, predictions, weights=missing_tokens)\n",
    "    #loss = tf.Print(\n",
    "    #    loss, [loss, labels, missing_tokens],\n",
    "    #    message='loss, labels, missing_tokens',\n",
    "    #    summarize=25,\n",
    "    #    first_n=25)\n",
    "    \n",
    "    loss = f.cross_entropy_loss(labels, predictions, weight=missing_tokens)\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss_matrix(gen_labels, gen_logits):\n",
    "    \"\"\"Computes the cross entropy loss for G.\n",
    "    Args:\n",
    "    gen_labels:  Labels for the correct token.\n",
    "    gen_logits: Generator logits.\n",
    "    Returns:\n",
    "    loss_matrix:  Loss matrix of shape [batch_size, sequence_length].\n",
    "    \"\"\"\n",
    "    loss = torch.sum(- target * F.log_softmax(logits, -1), -1)\n",
    "    mean_loss = loss.mean()\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def GAN_loss_matrix(dis_predictions):\n",
    "  \"\"\"Computes the cross entropy loss for G.\n",
    "  Args:\n",
    "    dis_predictions:  Discriminator predictions.\n",
    "  Returns:\n",
    "    loss_matrix: Loss matrix of shape [batch_size, sequence_length].\n",
    "  \"\"\"\n",
    "  eps = tf.constant(1e-7, tf.float32)\n",
    "  gan_loss_matrix = -tf.log(dis_predictions + eps)\n",
    "  return gan_loss_matrix\n",
    "\n",
    "\n",
    "def generator_GAN_loss(predictions):\n",
    "    \"\"\"Generator GAN loss based on Discriminator predictions.\"\"\"\n",
    "    return -torch.log(torch.mean(predictions))\n",
    "\n",
    "\n",
    "def generator_blended_forward_loss(gen_logits, gen_labels, dis_predictions,\n",
    "                                   is_real_input):\n",
    "  \"\"\"Computes the masked-loss for G.  This will be a blend of cross-entropy\n",
    "  loss where the true label is known and GAN loss where the true label has been\n",
    "  masked.\n",
    "  Args:\n",
    "    gen_logits: Generator logits.\n",
    "    gen_labels:  Labels for the correct token.\n",
    "    dis_predictions:  Discriminator predictions.\n",
    "    is_real_input:  Tensor indicating whether the label is present.\n",
    "  Returns:\n",
    "    loss: Scalar tf.float32 total loss.\n",
    "  \"\"\"\n",
    "  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=gen_labels, logits=gen_logits)\n",
    "  gan_loss = -tf.log(dis_predictions)\n",
    "  loss_matrix = tf.where(is_real_input, cross_entropy_loss, gan_loss)\n",
    "  return tf.reduce_mean(loss_matrix)\n",
    "\n",
    "\n",
    "def wasserstein_generator_loss(gen_logits, gen_labels, dis_values,\n",
    "                               is_real_input):\n",
    "  \"\"\"Computes the masked-loss for G.  This will be a blend of cross-entropy\n",
    "  loss where the true label is known and GAN loss where the true label is\n",
    "  missing.\n",
    "  Args:\n",
    "    gen_logits:  Generator logits.\n",
    "    gen_labels:  Labels for the correct token.\n",
    "    dis_values:  Discriminator values Tensor of shape [batch_size,\n",
    "      sequence_length].\n",
    "    is_real_input:  Tensor indicating whether the label is present.\n",
    "  Returns:\n",
    "    loss: Scalar tf.float32 total loss.\n",
    "  \"\"\"\n",
    "  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=gen_labels, logits=gen_logits)\n",
    "  # Maximize the dis_values (minimize the negative)\n",
    "  gan_loss = -dis_values\n",
    "  loss_matrix = tf.where(is_real_input, cross_entropy_loss, gan_loss)\n",
    "  loss = tf.reduce_mean(loss_matrix)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def wasserstein_discriminator_loss(real_values, fake_values):\n",
    "  \"\"\"Wasserstein discriminator loss.\n",
    "  Args:\n",
    "    real_values: Value given by the Wasserstein Discriminator to real data.\n",
    "    fake_values: Value given by the Wasserstein Discriminator to fake data.\n",
    "  Returns:\n",
    "    loss:  Scalar tf.float32 loss.\n",
    "  \"\"\"\n",
    "  real_avg = tf.reduce_mean(real_values)\n",
    "  fake_avg = tf.reduce_mean(fake_values)\n",
    "\n",
    "  wasserstein_loss = real_avg - fake_avg\n",
    "  return wasserstein_loss\n",
    "\n",
    "\n",
    "def wasserstein_discriminator_loss_intrabatch(values, is_real_input):\n",
    "  \"\"\"Wasserstein discriminator loss.  This is an odd variant where the value\n",
    "  difference is between the real tokens and the fake tokens within a single\n",
    "  batch.\n",
    "  Args:\n",
    "    values: Value given by the Wasserstein Discriminator of shape [batch_size,\n",
    "      sequence_length] to an imputed batch (real and fake).\n",
    "    is_real_input: tf.bool Tensor of shape [batch_size, sequence_length]. If\n",
    "      true, it indicates that the label is known.\n",
    "  Returns:\n",
    "    wasserstein_loss:  Scalar tf.float32 loss.\n",
    "  \"\"\"\n",
    "  zero_tensor = tf.constant(0., dtype=tf.float32, shape=[])\n",
    "\n",
    "  present = tf.cast(is_real_input, tf.float32)\n",
    "  missing = tf.cast(1 - present, tf.float32)\n",
    "\n",
    "  # Counts for real and fake tokens.\n",
    "  real_count = tf.reduce_sum(present)\n",
    "  fake_count = tf.reduce_sum(missing)\n",
    "\n",
    "  # Averages for real and fake token values.\n",
    "  real = tf.mul(values, present)\n",
    "  fake = tf.mul(values, missing)\n",
    "  real_avg = tf.reduce_sum(real) / real_count\n",
    "  fake_avg = tf.reduce_sum(fake) / fake_count\n",
    "\n",
    "  # If there are no real or fake entries in the batch, we assign an average\n",
    "  # value of zero.\n",
    "  real_avg = tf.where(tf.equal(real_count, 0), zero_tensor, real_avg)\n",
    "  fake_avg = tf.where(tf.equal(fake_count, 0), zero_tensor, fake_avg)\n",
    "\n",
    "  wasserstein_loss = real_avg - fake_avg\n",
    "  return wasserstein_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchnlp.datasets as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.datasets import imdb_dataset\n",
    "from torchnlp.datasets import penn_treebank_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 84.1MB [00:56, 1.49MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "train = imdb_dataset(train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The story of the boy thief of Bagdad (as it was once spelled) has attracted filmmakers from Raoul Walsh in 1924, who starred Douglas Fairbanks in the first, silent, rendering of \"Thief of Bagdad,\" to less imposing, more recent attempts. The best, however, remains 1940\\'s version which for its time was a startling, magical panoply of top quality special effects. Those effects still work their charm.<br /><br />No less than six directors are listed for the technicolor movie which starred Sabu as the boy thief, Abu, John Justin as the dreamily in love deposed monarch, Ahmad and June Duprez as the lovely princess sought by Ahmad and pursued by the evil vizier, Jaffar, played by a sinister Conrad Veidt. The giant genie is ably acted by Rex Ingram.<br /><br />Ahmad is treacherously deposed by Jaffar and when later arrested by that traitorous serpent, he and the boy, Abu, suffer what are clearly incapacitating fates. Ahmad is rendered blind and Abu becomes a lovable mutt. Their adventures through the gaily decorated Hollywood backlots are fun but the special effects make this film work.<br /><br />Two men were responsible for everything from a magic flying carpet to the gargantuan genie who pops out of a bottle with a tornado-like black swirl: Lawrence W. Butler and Tom Howard. (Howard, incidentally, did the special effects for the 1961 version of this film. Both men had long and distinguished careers in technical wizardry.)<br /><br />Duprez is outstandingly lovely while little called on for serious acting. Justin\\'s Ahmad projects a driven but dreamy romanticism untouched by erotic impulses. Sabu is really the central actor in many scenes and he\\'s very good. For a movie meant for kids as well as adults there\\'s a fair amount of violence but of the bloodless kind. Still, I don\\'t think anyone under eight ought to see \"Thief of Bagdad.\"<br /><br />This film makes periodic appearances on TV but today my teenage son and I saw it in a theater with quite a few youngsters present. It was great to see computer-besotted kids in an affluent community respond with cheers and applause to special effects that must seem primitive to them.<br /><br />\"Thief of Bagdad\" is a pre-war Hollywood classic from a time when strong production values often resulted in enduringly attractive and important releases. This is one of the best of its kind.<br /><br />9/10.',\n",
       " 'sentiment': 'pos'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
