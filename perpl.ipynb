{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def greedy_sample(logits):\n",
    "    batch_size, seq_len, _ = logits.size()\n",
    "    max_values, max_indices = logits.max(dim=2)\n",
    "    return max_indices\n",
    "\n",
    "def ppl(sequences, log_probs):\n",
    "    batch_size, seq_len = sequences.size()\n",
    "    seq_log_probs = torch.zeros_like(sequences).float()\n",
    "    for b in range(batch_size):\n",
    "        for t in range(seq_len):\n",
    "            idx = sequences[b, t].item()\n",
    "            seq_log_probs[b, t] = log_probs[b, t, idx].item()\n",
    "    return seq_log_probs.sum()\n",
    "\n",
    "def perplexity(truths, sampled, log_probs):\n",
    "    batch_size, seq_len, vocab_size = log_probs.size()\n",
    "    _ppl = {\n",
    "        'ground-truth': ppl(truths, log_probs).mean(),\n",
    "        'sampled': ppl(sampled, log_probs).mean(),\n",
    "    }\n",
    "    return _ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipped_value --- paramete rfor ClippedAdfam \n",
    "\n",
    "class RL(nn.Module):\n",
    "    def __init__(self, gamma, clip_value):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.log_sigmoid = torch.nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, log_probs, logits, weight, baseline=None):\n",
    "        batch_size, seqlen, _ = logits.size()\n",
    "        rewards = self.log_sigmoid(logits).squeeze(2)\n",
    "\n",
    "        cumulative_rewards = []\n",
    "        for t in range(seqlen):\n",
    "            cum_value = rewards.new_zeros(batch_size)\n",
    "            for s in range(t, seqlen):\n",
    "                exp = float(s-t)\n",
    "                k = (self.gamma ** exp)\n",
    "                cum_value +=  k * weight[:, s]  * rewards[:, s]\n",
    "            cumulative_rewards.append(cum_value)\n",
    "\n",
    "        cumulative_rewards = torch.stack(cumulative_rewards, dim=1)\n",
    "\n",
    "        if baseline is not None:\n",
    "            baseline = baseline.squeeze(2)\n",
    "            advantages = cumulative_rewards - baseline\n",
    "        else:\n",
    "            advantages = cumulative_rewards\n",
    "\n",
    "        advantages = advantages - advantages.mean(dim=0)\n",
    "        advantages = advantages.clamp(-1*self.clip_value, self.clip_value)\n",
    "        \n",
    "        generator_objective = (advantages * log_probs).sum(dim=0)\n",
    "        return (generator_objective, cumulative_rewards.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
