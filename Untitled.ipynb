{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#from fairseq.data.dictionary import Dictionary\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.precompute()\n",
    "\n",
    "    def precompute(self):\n",
    "        self.sample_files = []\n",
    "        dirs = ['pos', 'neg', 'unsup']\n",
    "        for _dir in dirs:\n",
    "            path = os.path.join(self.path, _dir)\n",
    "            for root, dirs, files in os.walk(path, topdown=False):\n",
    "               for name in files:\n",
    "                   fpath = os.path.join(root, name)\n",
    "                   self.sample_files.append(fpath)\n",
    "        self.length = len(self.sample_files)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fpath = self.sample_files[idx]\n",
    "        with open(fpath) as fp:\n",
    "            contents = fp.read()\n",
    "            ignores = ['<br>', '<br/>', '<br />']\n",
    "            for ignore in ignores:\n",
    "                contents = contents.replace(ignore, '')\n",
    "        return contents\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#from fairseq.data.dictionary import Dictionary\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from .imdb_enhanced import IMDbEnhancedDataset\n",
    "from .imdb_dataset import IMDbDataset\n",
    "from .vocab_builder import VocabBuilder\n",
    "\n",
    "class TensorIMDbDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, mask_builder, truncate_length, vocab=None):\n",
    "        self.path = path\n",
    "        self._dataset = IMDbEnhancedDataset(path, tokenizer, truncate_length)\n",
    "        self.mask_builder = mask_builder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.truncate_length = truncate_length\n",
    "        self.vocab = vocab\n",
    "        self._construct_vocabulary()\n",
    "\n",
    "    def _construct_vocabulary(self):\n",
    "        if self.vocab is None:\n",
    "            raw_dataset = IMDbDataset(self.path)\n",
    "            builder = VocabBuilder(raw_dataset, self.tokenizer, self.path)\n",
    "            self.vocab = builder.vocab()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self._dataset[idx]\n",
    "        sequence_length = min(self.truncate_length, len(tokens))\n",
    "        mask_idxs = self.mask_builder(sequence_length)\n",
    "        tokens = tokens[:sequence_length]\n",
    "\n",
    "        def get_pair(tokens, mask_idxs, mask_id):\n",
    "            idxs = [self.vocab.index(token) for token in tokens]\n",
    "\n",
    "            def _pad(ls, desired_length, pad_index):\n",
    "                padded_ls = deepcopy(ls)\n",
    "                while len(padded_ls) <= desired_length:\n",
    "                    padded_ls.append(pad_index)\n",
    "                return padded_ls\n",
    "\n",
    "            srcs = deepcopy(idxs)\n",
    "            srcs.append(self.vocab.eos())\n",
    "\n",
    "            tgts = deepcopy(idxs)\n",
    "            tgts.insert(0, self.vocab.eos())\n",
    "\n",
    "            srcs = _pad(srcs, self.truncate_length, self.vocab.pad())\n",
    "            tgts = _pad(tgts, self.truncate_length, self.vocab.pad())\n",
    "\n",
    "            mask = torch.zeros(len(tgts))\n",
    "            for mask_idx in mask_idxs:\n",
    "                offset = 1 # For eos\n",
    "                mask[mask_idx + offset] = 1\n",
    "                srcs[mask_idx] = mask_id\n",
    "\n",
    "            return (srcs, tgts, len(srcs), mask)\n",
    "\n",
    "        mask_id = self.vocab.index(self.mask_builder.mask_token)\n",
    "        return get_pair(tokens, mask_idxs, mask_id)\n",
    "\n",
    "\n",
    "    def get_collate_fn(self):\n",
    "        return TensorIMDbDataset.collate\n",
    "\n",
    "    @staticmethod\n",
    "    def collate(samples):\n",
    "        srcs, tgts, lengths, masks = list(zip(*samples))\n",
    "\n",
    "        srcs = torch.LongTensor(srcs)\n",
    "        tgts = torch.LongTensor(tgts)\n",
    "\n",
    "        lengths = torch.LongTensor(lengths)\n",
    "        lengths, sort_order = lengths.sort(descending=True)\n",
    "        \n",
    "        def _rearrange(tensor):\n",
    "            return tensor.index_select(0, sort_order)\n",
    "\n",
    "        srcs  = _rearrange(pad_sequence(srcs, batch_first=True))\n",
    "        tgts  = _rearrange(pad_sequence(tgts, batch_first=True))\n",
    "        masks = _rearrange(torch.stack(masks, dim=0))\n",
    "\n",
    "        return (srcs, tgts, lengths, masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "\n",
    "class Tokenizer(nn.Module):\n",
    "    pass\n",
    "\n",
    "class SpaceTokenizer(Tokenizer):\n",
    "    def forward(self, seq):\n",
    "        return seq.split()\n",
    "\n",
    "\n",
    "class SentencePieceTokenizer:\n",
    "    def __init__(self, model_prefix):\n",
    "        self.prefix = model_prefix\n",
    "\n",
    "        self.path = {}\n",
    "        for key in ['model', 'vocab']:\n",
    "            self.path[key] = '{}.{}'.format(self.prefix, key)\n",
    "\n",
    "        self.sp = spm.SentencePieceProcessor() \n",
    "        self.sp.Load(self.path['model'])\n",
    "\n",
    "        # Build vocabulary.\n",
    "        self.build_vocabulary()\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        self.vocab = set()\n",
    "        for line in open(self.path['vocab']):\n",
    "            word, score = line.strip().split()\n",
    "            self.vocab.add(word)\n",
    "\n",
    "\n",
    "    def __call__(self, text):\n",
    "        tokens = self.sp.EncodeAsPieces(text)\n",
    "\n",
    "        to_utf = lambda x: x.decode(\"utf-8\") \n",
    "        stokens = list(map(to_utf, tokens))\n",
    "\n",
    "        wanted = lambda s: s in self.vocab\n",
    "        stokens = list(filter(wanted, stokens))\n",
    "        return stokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'num_rollouts' : 1, 'path' : 'data/aclImdb_v1.tar.gz', 'max_epochs' : 10, 'validate_every' : 5, 'num_rollouts' : 5, 'criterion' : 'dummy', 'spm_prefix' : 'data/aclImdb/train/imdb'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sequence_length, batch_size=None, is_present=0.7):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    returns: [1, 1, 0, 1, 0, 1]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        mask = np.random.binomial(1, is_present, size=(batch_size, sequence_length))\n",
    "    elif batch_size is None:\n",
    "        mask = np.random.binomial(1, is_present, size=(sequence_length,))\n",
    "    return torch.from_numpy(mask).long()\n",
    "\n",
    "class Mask:\n",
    "    mask_token = '__<m>__'\n",
    "    def __call__(self, n):\n",
    "        idxs = self.forward(n)\n",
    "\n",
    "        # Verify indices are okay.\n",
    "        assert ( len(idxs) < n)\n",
    "        valid_set = set(list(range(n)))\n",
    "        for i in idxs:\n",
    "            assert(i in valid_set)\n",
    "\n",
    "        return idxs\n",
    "    \n",
    "class ContiguousRandom(Mask):\n",
    "    def __init__(self, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.r = random.Random(42)\n",
    "\n",
    "    def forward(self, n):\n",
    "        n_chars = self.n_chars\n",
    "        start = self.r.randint(1, n-n_chars-1)\n",
    "        assert ( start + n_chars <= n)\n",
    "        idxs = []\n",
    "        for i in range(start, start+n_chars):\n",
    "            idxs.append(i)\n",
    "        return idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Internal: /sentencepiece/src/sentencepiece_processor.cc(73) [model_proto->ParseFromArray(serialized.data(), serialized.size())] ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-acc056e4c9f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspm_tokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentencePieceTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spm_prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Compute Batch Size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_tokens_per_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m48000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# max_tokens_per_device = 1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-a84a666a55a5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_prefix)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Build vocabulary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/anaconda/envs/py35/lib/python3.5/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mLoad\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_Load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadOrDie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Internal: /sentencepiece/src/sentencepiece_processor.cc(73) [model_proto->ParseFromArray(serialized.data(), serialized.size())] "
     ]
    }
   ],
   "source": [
    "spm_tokenize = SentencePieceTokenizer(args['spm_prefix'])\n",
    "\n",
    "# Compute Batch Size\n",
    "max_tokens_per_device = 48000\n",
    "# max_tokens_per_device = 1000\n",
    "n_devices = torch.cuda.device_count()\n",
    "max_tokens = max_tokens_per_device * n_devices\n",
    "truncate_length = 20\n",
    "batch_size = int(max_tokens/truncate_length)\n",
    "\n",
    "#checkpoint_path = \"/home/jerin/mgan-attempts/\"\n",
    "#saver = Saver(checkpoint_path)\n",
    "\n",
    "train_path = os.path.join(args['path'], 'train')\n",
    "dev_path = os.path.join(args['path'], 'test')\n",
    "\n",
    "train_dataset = TensorIMDbDataset(\n",
    "        train_path, spm_tokenize, \n",
    "        rmask, truncate_length\n",
    ")\n",
    "\n",
    "# Constructed vocabulary from train\n",
    "vocab = train_dataset.vocab\n",
    "Task = namedtuple('Task', 'source_dictionary target_dictionary')\n",
    "task = Task(source_dictionary=vocab, \n",
    "        target_dictionary=vocab)\n",
    "\n",
    "trainer = MGANTrainer(args, task, saver, visdom, vocab)\n",
    "def loader(dataset):\n",
    "    _loader = DataLoader(dataset, batch_size=batch_size, \n",
    "            collate_fn=TensorIMDbDataset.collate, \n",
    "            shuffle=True, num_workers=8)\n",
    "    return _loader\n",
    "\n",
    "#trainer.validate_dataset(loader(train_dataset))\n",
    "\n",
    "dev_dataset = TensorIMDbDataset(\n",
    "        dev_path, spm_tokenize, \n",
    "        rmask, truncate_length,\n",
    "        vocab \n",
    ")\n",
    "\n",
    "Datasets = namedtuple('Dataset', 'train dev')\n",
    "datasets = Datasets(\n",
    "        train=train_dataset,\n",
    "        dev=dev_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
