{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired from PyTorch Tutorial [Translation with a Sequence to Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchnlp.datasets import imdb_dataset\n",
    "from torchnlp.datasets import penn_treebank_dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MASKED_token = 2\n",
    "MAX_LENGTH = 42\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 1, \"MASKEDTOKEN\": 2}\n",
    "        self.index2word = {0: \"SOSTOKEN\", 1: \"EOSTOKEN\", 2: \"MASKEDTOKEN\"}\n",
    "        self.word2count = {\"SOSTOKEN\": 0, \"EOSTOKEN\": 0, \"MASKEDTOKEN\": 0}\n",
    "        \n",
    "        self.n_words = 3  # Count SOS and EOS and Masked token\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    \"\"\"\n",
    "    Turn a Unicode string to plain ASCII, thanks to\n",
    "    https://stackoverflow.com/a/518232/2809427\n",
    "    \"\"\"\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):  # Lowercase, trim, and remove non-letter characters\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    #s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z]+\", r\" \", s)\n",
    "    s = \" \".join(s.split()[:40])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLang(dataset_title):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dataset_title: either 'imdb' or 'ptb'\n",
    "    \"\"\"\n",
    "    print(\"Reading lines...\")\n",
    "    if dataset_title == 'imdb':\n",
    "        train = imdb_dataset(train=True, directory='../data/')\n",
    "        # Read the dataset and split into lines\n",
    "        lines = [train[ind]['text'].strip() for ind, doc in enumerate(train)]\n",
    "        # Normalize lines\n",
    "        lines = [' '.join([\"SOSTOKEN\", normalizeString(s), \"EOSTOKEN\"]) for s in lines]\n",
    "        lang = Lang(dataset_title)\n",
    "    elif dataset_title == 'ptb':\n",
    "        raise NotImplementedError\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(dataset_title):\n",
    "    lang, lines = readLang(dataset_title)\n",
    "    print(\"Read %s sentence pairs\" % len(lines))\n",
    "    print(\"Counting words...\")\n",
    "    for l in lines:\n",
    "        lang.addSentence(l)\n",
    "    print(\"Counted words:\")\n",
    "    print(lang.name, lang.n_words)\n",
    "    return lang, lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.relu(output)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(1, 1, self.hidden_size, device=device),\n",
    "                torch.zeros(1, 1, self.hidden_size, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #print(input, embedded.shape, hidden[0].shape, encoder_outputs.shape)\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "        #print(embedded.shape, attn_applied.shape)\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(sequence_length, batch_size=None, is_present=0.7):\n",
    "    \"\"\"\n",
    "    e.g.\n",
    "    returns: [1, 1, 0, 1, 0, 1]\n",
    "    \"\"\"\n",
    "    if batch_size is not None:\n",
    "        mask = np.random.binomial(1, is_present, size=(batch_size, sequence_length))\n",
    "    elif batch_size is None:\n",
    "        mask = np.random.binomial(1, is_present, size=(sequence_length,))\n",
    "    return torch.from_numpy(mask).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_with_is_missing_token(inputs, targets_present, masked_value=\"MASKEDTOKEN\"):\n",
    "    \"\"\"\n",
    "    e.g. \n",
    "        inputs = [a, b, c, d, e]\n",
    "        targets = [b, c, d, e, f]\n",
    "        targets_present = [1, 0, 1, 0, 1]\n",
    "        masked_value = <m>\n",
    "        \n",
    "    then,\n",
    "        transformed_input = [a, b, <m>, d, <m>]\n",
    "        \n",
    "    Args:\n",
    "        inputs: tensor with shape [sequence_length] with tokens\n",
    "        targets_present: tensor with shape [sequence_length] with 1. representing presence of a word of dtype=torch.long\n",
    "        \n",
    "    from github.com/tensorflow/models/blob/master/research/maskgan\n",
    "    \"\"\"\n",
    "    transformed_input = [inputs[0]]\n",
    "    for ind, word in enumerate(inputs[1:]):\n",
    "        if targets_present[ind] == 1:\n",
    "            transformed_input.append(word)\n",
    "        elif targets_present[ind] == 0:\n",
    "            transformed_input.append(masked_value)\n",
    "    transformed_input[-1] = inputs[-1]\n",
    "    return transformed_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'MASKEDTOKEN', 'c', 'MASKEDTOKEN', 'e', 'MASKEDTOKEN', 'MASKEDTOKEN', 'h', 'MASKEDTOKEN', 'j', 'MASKEDTOKEN', 'MASKEDTOKEN', 'm', 'MASKEDTOKEN', 'o', 'MASKEDTOKEN', 'MASKEDTOKEN', 'r', 'MASKEDTOKEN', 't', 'MASKEDTOKEN', 'MASKEDTOKEN', 'w', 'MASKEDTOKEN', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(transform_input_with_is_missing_token(list(string.ascii_lowercase),\n",
    "                                      [(ind % 5) % 2 for ind, letter in enumerate(string.ascii_lowercase)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    #indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsForTrain(lang, sentence):\n",
    "    mask = generate_mask(len(sentence))\n",
    "    target_tensor = tensorFromSentence(lang, sentence)\n",
    "    transformed_sentence = \" \".join(transform_input_with_is_missing_token(sentence.split(), mask))\n",
    "    input_tensor = tensorFromSentence(lang, transformed_sentence)\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def indexFromTensor(lang, decoder_output):\n",
    "    return decoder_output.max(0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 42 # max(map(lambda x: len(x.split()), imdb_lines)) == 2516\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, lang, criterion, max_length=MAX_LENGTH):\n",
    "    #c_ = time()\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    \n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        \n",
    "    decoder_input = input_tensor[0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    for di in range(input_length):        \n",
    "        #print(encoder_outputs.shape)\n",
    "        decoder_output, decoder_hidden, decoder_attention  = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)      \n",
    "        loss += criterion(decoder_output, target_tensor[di + 1])\n",
    "\n",
    "        if input_tensor[di + 1].item() == MASKED_token:\n",
    "            #topv, topi = decoder_output.topk(1)\n",
    "            #decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            #print(decoder_output.min())\n",
    "            token_sample = torch.multinomial(torch.exp(decoder_output), 1)\n",
    "            decoder_input = token_sample.squeeze().detach()\n",
    "        else:\n",
    "            decoder_input = input_tensor[di + 1]\n",
    "\n",
    "        if input_tensor[di + 1].item() == EOS_token:\n",
    "            break\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, lang, lines, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    #start = time.time()\n",
    "    start = time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    #training_pairs = [tensorsForTrain(lang, random.choice(lines)) for i in range(n_iters)]\n",
    "    training_pairs = [tensorsForTrain(lang, random.choice(lines)) for i in range(n_iters)]\n",
    "    \n",
    "    criterion = nn.NLLLoss() # replaced NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        #c_ = time()\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        #print('Pairs created ...', time() - c_)\n",
    "        #c_ = time()\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, lang, criterion)\n",
    "        #print('Loss is done...', time() - c_)\n",
    "        #c_ = time()\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting results\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def showPlot(points):\n",
    "    #print(points)\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.grid()\n",
    "    plt.show();\n",
    "    print('plot was shown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "=========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def evaluate(encoder, decoder, input_lang, input_tensor, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        #decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "        decoder_input = input_tensor[0]\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = ['SOSTOKEN']\n",
    "        #decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):        \n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)      \n",
    "\n",
    "            if input_tensor[di + 1].item() == MASKED_token:\n",
    "                #topv, topi = decoder_output.topk(1)\n",
    "                #decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                #print(decoder_output.min())\n",
    "                token_sample = torch.multinomial(torch.exp(decoder_output), 1)\n",
    "                decoder_input = token_sample.squeeze().detach()\n",
    "                decoded_words.append(input_lang.index2word[decoder_input.item()].upper())\n",
    "            else:\n",
    "                decoder_input = input_tensor[di + 1]\n",
    "                decoded_words.append(input_lang.index2word[decoder_input.item()])\n",
    "            \n",
    "            \n",
    "            if input_tensor[di + 1].item() == EOS_token:\n",
    "                decoded_words.append('EOSTOKEN')\n",
    "                break\n",
    "                \n",
    "        return decoded_words\n",
    "    #, decoder_attentions[:di + 1]\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def evaluateRandomly(encoder, decoder, input_lang, n=10):\n",
    "    for i in range(n):\n",
    "        \n",
    "        sentence = random.choice(imdb_lines)\n",
    "        pair = tensorsForTrain(input_lang, sentence)\n",
    "        #print('masked:\\n', *list(map(lambda x: input_lang.index2word[x.item()], pair[0])))\n",
    "        print('real:\\n', sentence)\n",
    "        #print('filled:', *list(map(lambda x: input_lang.index2word[x.item()], \n",
    "        #                           evaluate(encoder, decoder, input_lang, sentence))))\n",
    "        #mask = generate_mask(len(sentence.split()), is_present=0.8)\n",
    "        print('filled:\\n', *evaluate(encoder, decoder, input_lang, \n",
    "                                     pair[1]))\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = 'imdb'\n",
    "lang_filename = './data/' + dataset + '_lang.pkl'\n",
    "if os.path.exists(lang_filename):\n",
    "    with open(lang_filename, 'rb') as file:\n",
    "        (lang, lines) = pkl.load(file)\n",
    "else:\n",
    "    lang, lines = prepareData(dataset)\n",
    "    with open(lang_filename, 'wb') as file:\n",
    "        pkl.dump((lang, lines), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
    "#decoder1 = DecoderRNN(hidden_size, imdb_lang.n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_all_buffers',\n",
       " '_all_weights',\n",
       " '_apply',\n",
       " '_backend',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_data_ptrs',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_modules',\n",
       " '_parameters',\n",
       " '_slow_forward',\n",
       " '_tracing_name',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'all_weights',\n",
       " 'apply',\n",
       " 'batch_first',\n",
       " 'bias',\n",
       " 'bias_hh_l0',\n",
       " 'bias_ih_l0',\n",
       " 'bidirectional',\n",
       " 'check_forward_args',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dropout_state',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'flatten_parameters',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'hidden_size',\n",
       " 'input_size',\n",
       " 'load_state_dict',\n",
       " 'mode',\n",
       " 'modules',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_layers',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'reset_parameters',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'weight_hh_l0',\n",
       " 'weight_ih_l0',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(encoder1.lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 2s (- 0m 25s) (2 10%) 10.1351\n",
      "0m 5s (- 0m 22s) (4 20%) 10.1363\n",
      "0m 8s (- 0m 19s) (6 30%) 10.1121\n",
      "0m 10s (- 0m 15s) (8 40%) 10.0048\n",
      "0m 12s (- 0m 12s) (10 50%) 10.1019\n",
      "0m 15s (- 0m 10s) (12 60%) 10.0990\n",
      "0m 17s (- 0m 7s) (14 70%) 10.1015\n",
      "0m 20s (- 0m 5s) (16 80%) 10.0723\n",
      "0m 22s (- 0m 2s) (18 90%) 10.0054\n",
      "0m 24s (- 0m 0s) (20 100%) 9.9687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot was shown\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainIters(encoder1, attn_decoder1, lang, lines, 20, print_every=2, plot_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 6398282\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    return sum([np.prod(p.size()) for p in model_parameters])\n",
    "\n",
    "print(\"Total number of trainable parameters:\", count_parameters(encoder1) + count_parameters(attn_decoder1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real:\n",
      " SOSTOKEN nothin there ain t nothing in room but you ain t got no business going in there anyway so stay out you understand stay out br br never has there been such a feat of psychological horror as this film EOSTOKEN\n",
      "filled:\n",
      " SOSTOKEN nothin there ain t nothing in room but you ain t got no business going in there anyway so stay out you understand stay out br br never has there been such a feat of psychological horror as this film EOSTOKEN EOSTOKEN\n",
      "real:\n",
      " SOSTOKEN deaf secretary carla emmanuelle devos is bullied by her mean spirited male colleagues br br when they suggest she needs an assistant it seems like the final insult but when the first applicant is ex con paul vincent cassel she EOSTOKEN\n",
      "filled:\n",
      " SOSTOKEN deaf secretary carla emmanuelle devos is bullied by her mean spirited male colleagues br br when they suggest she needs an assistant it seems like the final insult but when the first applicant is ex con paul vincent cassel she EOSTOKEN EOSTOKEN\n",
      "real:\n",
      " SOSTOKEN i really felt cheated after seeing this picture it felt like i sat watching this movie minutes for nothing i don t understand what they were thinking when they made this it hardly gets into jeffrey dahmer murdering and it EOSTOKEN\n",
      "filled:\n",
      " SOSTOKEN i really felt cheated after seeing this picture it felt like i sat watching this movie minutes for nothing i don t understand what they were thinking when they made this it hardly gets into jeffrey dahmer murdering and it EOSTOKEN EOSTOKEN\n",
      "real:\n",
      " SOSTOKEN in northeastern of brazil the father of the twelve years old illiterate maria fernanda carvalho sells his daughter to the middle man of a prostitution organization tadeu chico dias to be employed as a housemaid and have a better life EOSTOKEN\n",
      "filled:\n",
      " SOSTOKEN in northeastern of brazil the father of the twelve years old illiterate maria fernanda carvalho sells his daughter to the middle man of a prostitution organization tadeu chico dias to be employed as a housemaid and have a better life EOSTOKEN EOSTOKEN\n",
      "real:\n",
      " SOSTOKEN a missed train a wrong phone number an extra cup of coffee what happens to those around you when you make a seemingly innocuous decision most people don t give it a thought as they absorbed in their own thoughts EOSTOKEN\n",
      "filled:\n",
      " SOSTOKEN a missed train a wrong phone number an extra cup of coffee what happens to those around you when you make a seemingly innocuous decision most people don t give it a thought as they absorbed in their own thoughts EOSTOKEN EOSTOKEN\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, decoder1, imdb_lang, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
